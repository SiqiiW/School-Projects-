{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiqiiW/School-Projects-/blob/main/ShareGPT4V_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "outputId": "4e155db8-68c6-4340-b933-a79c0258aacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'InternLM-XComposer' already exists and is not an empty directory.\n",
            "\u001b[31mERROR: Invalid requirement: 'llava==ShareGPT4V': Expected end or semicolon (after name and no valid version specifier)\n",
            "    llava==ShareGPT4V\n",
            "         ^\u001b[0m\u001b[31m\n",
            "\u001b[0m/content/InternLM-XComposer/projects/ShareGPT4V\n",
            "Requirement already satisfied: llava in /usr/local/lib/python3.11/dist-packages (1.2.2.post1)\n",
            "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.11/dist-packages (from llava) (2.1.2)\n",
            "Requirement already satisfied: torchvision==0.16.2 in /usr/local/lib/python3.11/dist-packages (from llava) (0.16.2)\n",
            "Requirement already satisfied: transformers==4.37.2 in /usr/local/lib/python3.11/dist-packages (from llava) (4.37.2)\n",
            "Requirement already satisfied: tokenizers==0.15.1 in /usr/local/lib/python3.11/dist-packages (from llava) (0.15.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.11/dist-packages (from llava) (0.1.99)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.11/dist-packages (from llava) (1.0.13)\n",
            "Requirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.11/dist-packages (from llava) (0.21.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from llava) (0.14.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from llava) (0.45.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llava) (2.10.6)\n",
            "Requirement already satisfied: markdown2[all] in /usr/local/lib/python3.11/dist-packages (from llava) (2.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llava) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (from llava) (1.2.2)\n",
            "Requirement already satisfied: gradio==4.16.0 in /usr/local/lib/python3.11/dist-packages (from llava) (4.16.0)\n",
            "Requirement already satisfied: gradio_client==0.8.1 in /usr/local/lib/python3.11/dist-packages (from llava) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from llava) (2.32.3)\n",
            "Requirement already satisfied: httpx==0.24.0 in /usr/local/lib/python3.11/dist-packages (from llava) (0.24.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llava) (0.34.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llava) (0.115.8)\n",
            "Requirement already satisfied: einops==0.6.1 in /usr/local/lib/python3.11/dist-packages (from llava) (0.6.1)\n",
            "Requirement already satisfied: einops-exts==0.0.4 in /usr/local/lib/python3.11/dist-packages (from llava) (0.0.4)\n",
            "Requirement already satisfied: timm==0.6.13 in /usr/local/lib/python3.11/dist-packages (from llava) (0.6.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0->llava) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0->llava) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0->llava) (6.0.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (5.5.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (0.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (3.10.15)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (10.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (0.9.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.9 in /usr/local/lib/python3.11/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio_client==0.8.1->llava) (2024.10.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio_client==0.8.1->llava) (11.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.24.0->llava) (2025.1.31)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from httpx==0.24.0->llava) (0.17.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.24.0->llava) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.24.0->llava) (1.3.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->llava) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->llava) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->llava) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (3.17.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->llava) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->llava) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->llava) (4.67.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->llava) (12.4.127)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llava) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->llava) (2.27.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llava) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llava) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->llava) (0.45.3)\n",
            "Requirement already satisfied: pygments>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from markdown2[all]->llava) (2.18.0)\n",
            "Requirement already satisfied: wavedrom in /usr/local/lib/python3.11/dist-packages (from markdown2[all]->llava) (2.0.3.post3)\n",
            "Requirement already satisfied: latex2mathml in /usr/local/lib/python3.11/dist-packages (from markdown2[all]->llava) (3.77.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->llava) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->llava) (2.3.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.16.0->llava) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.16.0->llava) (1.27.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava) (2025.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.16.0->llava) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.16.0->llava) (13.9.4)\n",
            "\u001b[33mWARNING: typer 0.15.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2->llava) (1.3.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.11/dist-packages (from wavedrom->markdown2[all]->llava) (1.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from wavedrom->markdown2[all]->llava) (1.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava) (0.22.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.16.0->llava) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.16.0->llava) (0.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/usr/local/lib/python3.11/dist-packages/llava/model/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f825cb9272dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_IM_END_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_IM_START_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_IMAGE_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_TOKEN_INDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeparatorStyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_templates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_conversation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeywordsStoppingCriteria\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_image_from_base64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_image_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llava/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlavaLlamaForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/usr/local/lib/python3.11/dist-packages/llava/model/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/InternLM-XComposer\n",
        "!pip install -q https://github.com/camenduru/wheels/releases/download/colab/llava-ShareGPT4V-1.1.3-py3-none-any.whl gradio\n",
        "%cd /content/InternLM-XComposer/projects/ShareGPT4V\n",
        "\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from threading import Thread\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "from llava.constants import (DEFAULT_IM_END_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX)\n",
        "from llava.conversation import (SeparatorStyle, conv_templates, default_conversation)\n",
        "from llava.mm_utils import (KeywordsStoppingCriteria, load_image_from_base64, process_images, tokenizer_image_token)\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from transformers import TextIteratorStreamer\n",
        "\n",
        "print(gr.__version__)\n",
        "\n",
        "block_css = \"\"\"\n",
        "\n",
        "#buttons button {\n",
        "    min-width: min(120px,100%);\n",
        "}\n",
        "\"\"\"\n",
        "title_markdown = (\"\"\"\n",
        "# üê¨ ShareGPT4V: Improving Large Multi-modal Models with Better Captions\n",
        "### üîä Notice: The demo of Share-Captioner will soon be supported. Stay tune for updates!\n",
        "[[Project Page](https://sharegpt4v.github.io/)] [[Code](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V)] | üìö [[Paper](https://arxiv.org/pdf/2311.12793.pdf)]\n",
        "\"\"\")\n",
        "tos_markdown = (\"\"\"\n",
        "### Terms of use\n",
        "By using this service, users are required to agree to the following terms:\n",
        "The service is a research preview intended for non-commercial use only. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes.\n",
        "For an optimal experience, please use desktop computers for this demo, as mobile devices may compromise its quality.\n",
        "\"\"\")\n",
        "learn_more_markdown = (\"\"\"\n",
        "### License\n",
        "The service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.\n",
        "\"\"\")\n",
        "ack_markdown = (\"\"\"\n",
        "### Acknowledgement\n",
        "The template for this web demo is from [LLaVA](https://github.com/haotian-liu/LLaVA), and we are very grateful to LLaVA for their open source contributions to the community!\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "def regenerate(state, image_process_mode):\n",
        "    state.messages[-1][-1] = None\n",
        "    prev_human_msg = state.messages[-2]\n",
        "    if type(prev_human_msg[1]) in (tuple, list):\n",
        "        prev_human_msg[1] = (*prev_human_msg[1][:2], image_process_mode)\n",
        "    state.skip_next = False\n",
        "    return (state, state.to_gradio_chatbot(), \"\", None)\n",
        "\n",
        "\n",
        "def clear_history():\n",
        "    state = default_conversation.copy()\n",
        "    return (state, state.to_gradio_chatbot(), \"\", None)\n",
        "\n",
        "\n",
        "def add_text(state, text, image, image_process_mode):\n",
        "    if len(text) <= 0 and image is None:\n",
        "        state.skip_next = True\n",
        "        return (state, state.to_gradio_chatbot(), \"\", None)\n",
        "\n",
        "    text = text[:1536]  # Hard cut-off\n",
        "    if image is not None:\n",
        "        text = text[:1200]  # Hard cut-off for images\n",
        "        if '<image>' not in text:\n",
        "            # text = '<Image><image></Image>' + text\n",
        "            text = text + '\\n<image>'\n",
        "        text = (text, image, image_process_mode)\n",
        "        if len(state.get_images(return_pil=True)) > 0:\n",
        "            state = default_conversation.copy()\n",
        "    state.append_message(state.roles[0], text)\n",
        "    state.append_message(state.roles[1], None)\n",
        "    state.skip_next = False\n",
        "    return (state, state.to_gradio_chatbot(), \"\", None)\n",
        "\n",
        "\n",
        "def load_demo():\n",
        "    state = default_conversation.copy()\n",
        "    return state\n",
        "\n",
        "@torch.inference_mode()\n",
        "def get_response(params):\n",
        "    prompt = params[\"prompt\"]\n",
        "    ori_prompt = prompt\n",
        "    images = params.get(\"images\", None)\n",
        "    num_image_tokens = 0\n",
        "    if images is not None and len(images) > 0:\n",
        "        if len(images) > 0:\n",
        "            if len(images) != prompt.count(DEFAULT_IMAGE_TOKEN):\n",
        "                raise ValueError(\n",
        "                    \"Number of images does not match number of <image> tokens in prompt\")\n",
        "\n",
        "            images = [load_image_from_base64(image) for image in images]\n",
        "            images = process_images(images, image_processor, model.config)\n",
        "\n",
        "            if type(images) is list:\n",
        "                images = [image.to(model.device, dtype=torch.float16)\n",
        "                          for image in images]\n",
        "            else:\n",
        "                images = images.to(model.device, dtype=torch.float16)\n",
        "\n",
        "            replace_token = DEFAULT_IMAGE_TOKEN\n",
        "            if getattr(model.config, 'mm_use_im_start_end', False):\n",
        "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
        "            prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
        "\n",
        "            num_image_tokens = prompt.count(\n",
        "                replace_token) * model.get_vision_tower().num_patches\n",
        "        else:\n",
        "            images = None\n",
        "        image_args = {\"images\": images}\n",
        "    else:\n",
        "        images = None\n",
        "        image_args = {}\n",
        "\n",
        "    temperature = float(params.get(\"temperature\", 1.0))\n",
        "    top_p = float(params.get(\"top_p\", 1.0))\n",
        "    max_context_length = getattr(\n",
        "        model.config, 'max_position_embeddings', 2048)\n",
        "    max_new_tokens = min(int(params.get(\"max_new_tokens\", 256)), 1024)\n",
        "    stop_str = params.get(\"stop\", None)\n",
        "    do_sample = True if temperature > 0.001 else False\n",
        "\n",
        "    input_ids = tokenizer_image_token(\n",
        "        prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
        "    keywords = [stop_str]\n",
        "    stopping_criteria = KeywordsStoppingCriteria(\n",
        "        keywords, tokenizer, input_ids)\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=15)\n",
        "\n",
        "    max_new_tokens = min(max_new_tokens, max_context_length -\n",
        "                         input_ids.shape[-1] - num_image_tokens)\n",
        "\n",
        "    if max_new_tokens < 1:\n",
        "        yield json.dumps({\"text\": ori_prompt + \"Exceeds max token length. Please start a new conversation, thanks.\", \"error_code\": 0}).encode() + b\"\\0\"\n",
        "        return\n",
        "\n",
        "    # local inference\n",
        "    thread = Thread(target=model.generate, kwargs=dict(\n",
        "        inputs=input_ids,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        streamer=streamer,\n",
        "        stopping_criteria=[stopping_criteria],\n",
        "        use_cache=True,\n",
        "        **image_args\n",
        "    ))\n",
        "    thread.start()\n",
        "\n",
        "    generated_text = ori_prompt\n",
        "    for new_text in streamer:\n",
        "        generated_text += new_text\n",
        "        if generated_text.endswith(stop_str):\n",
        "            generated_text = generated_text[:-len(stop_str)]\n",
        "        yield json.dumps({\"text\": generated_text, \"error_code\": 0}).encode()\n",
        "\n",
        "\n",
        "def http_bot(state, temperature, top_p, max_new_tokens):\n",
        "    if state.skip_next:\n",
        "        # This generate call is skipped due to invalid inputs\n",
        "        yield (state, state.to_gradio_chatbot())\n",
        "        return\n",
        "\n",
        "    if len(state.messages) == state.offset + 2:\n",
        "        # First round of conversation\n",
        "        if \"llava\" in model_name.lower():\n",
        "            if 'llama-2' in model_name.lower():\n",
        "                template_name = \"llava_llama_2\"\n",
        "            elif \"v1\" in model_name.lower():\n",
        "                if 'mmtag' in model_name.lower():\n",
        "                    template_name = \"v1_mmtag\"\n",
        "                elif 'plain' in model_name.lower() and 'finetune' not in model_name.lower():\n",
        "                    template_name = \"v1_mmtag\"\n",
        "                else:\n",
        "                    template_name = \"llava_v1\"\n",
        "            elif \"mpt\" in model_name.lower():\n",
        "                template_name = \"mpt\"\n",
        "            else:\n",
        "                if 'mmtag' in model_name.lower():\n",
        "                    template_name = \"v0_mmtag\"\n",
        "                elif 'plain' in model_name.lower() and 'finetune' not in model_name.lower():\n",
        "                    template_name = \"v0_mmtag\"\n",
        "                else:\n",
        "                    template_name = \"llava_v0\"\n",
        "        elif \"mpt\" in model_name:\n",
        "            template_name = \"mpt_text\"\n",
        "        elif \"llama-2\" in model_name:\n",
        "            template_name = \"llama_2\"\n",
        "        else:\n",
        "            template_name = \"vicuna_v1\"\n",
        "        new_state = conv_templates[template_name].copy()\n",
        "        new_state.append_message(new_state.roles[0], state.messages[-2][1])\n",
        "        new_state.append_message(new_state.roles[1], None)\n",
        "        state = new_state\n",
        "\n",
        "    # Construct prompt\n",
        "    prompt = state.get_prompt()\n",
        "\n",
        "    all_images = state.get_images(return_pil=True)\n",
        "    all_image_hash = [hashlib.md5(image.tobytes()).hexdigest()\n",
        "                      for image in all_images]\n",
        "\n",
        "    # Make requests\n",
        "    pload = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": prompt,\n",
        "        \"temperature\": float(temperature),\n",
        "        \"top_p\": float(top_p),\n",
        "        \"max_new_tokens\": min(int(max_new_tokens), 1536),\n",
        "        \"stop\": state.sep if state.sep_style in [SeparatorStyle.SINGLE, SeparatorStyle.MPT] else state.sep2,\n",
        "        \"images\": f'List of {len(state.get_images())} images: {all_image_hash}',\n",
        "    }\n",
        "\n",
        "    pload['images'] = state.get_images()\n",
        "\n",
        "    state.messages[-1][-1] = \"‚ñå\"\n",
        "    yield (state, state.to_gradio_chatbot())\n",
        "\n",
        "    # for stream\n",
        "    output = get_response(pload)\n",
        "    for chunk in output:\n",
        "        if chunk:\n",
        "            data = json.loads(chunk.decode())\n",
        "            if data[\"error_code\"] == 0:\n",
        "                output = data[\"text\"][len(prompt):].strip()\n",
        "                state.messages[-1][-1] = output + \"‚ñå\"\n",
        "                yield (state, state.to_gradio_chatbot())\n",
        "            else:\n",
        "                output = data[\"text\"] + \\\n",
        "                    f\" (error_code: {data['error_code']})\"\n",
        "                state.messages[-1][-1] = output\n",
        "                yield (state, state.to_gradio_chatbot())\n",
        "                return\n",
        "            time.sleep(0.03)\n",
        "\n",
        "    state.messages[-1][-1] = state.messages[-1][-1][:-1]\n",
        "    yield (state, state.to_gradio_chatbot())\n",
        "\n",
        "\n",
        "def build_demo():\n",
        "    textbox = gr.Textbox(\n",
        "        show_label=False, placeholder=\"Enter text and press ENTER\", container=False)\n",
        "    with gr.Blocks(title=\"ShareGPT4V\", theme=gr.themes.Default(), css=block_css) as demo:\n",
        "        state = gr.State()\n",
        "        gr.Markdown(title_markdown)\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=5):\n",
        "                with gr.Row(elem_id=\"Model ID\"):\n",
        "                    gr.Dropdown(\n",
        "                        choices=['ShareGPT4V-7B'],\n",
        "                        value='ShareGPT4V-7B',\n",
        "                        interactive=True,\n",
        "                        label='Model ID',\n",
        "                        container=False)\n",
        "                imagebox = gr.Image(type=\"pil\")\n",
        "                image_process_mode = gr.Radio(\n",
        "                    [\"Crop\", \"Resize\", \"Pad\", \"Default\"],\n",
        "                    value=\"Default\",\n",
        "                    label=\"Preprocess for non-square image\", visible=False)\n",
        "\n",
        "                cur_dir = \"/content/InternLM-XComposer/projects/ShareGPT4V\"\n",
        "                gr.Examples(examples=[\n",
        "                    [f\"{cur_dir}/examples/breaking_bad.png\",\n",
        "                        \"What is the most common catchphrase of the character on the right?\"],\n",
        "                    [f\"{cur_dir}/examples/photo.png\",\n",
        "                        \"From a photography perspective, analyze what makes this picture beautiful?\"],\n",
        "                ], inputs=[imagebox, textbox])\n",
        "\n",
        "                with gr.Accordion(\"Parameters\", open=False) as _:\n",
        "                    temperature = gr.Slider(\n",
        "                        minimum=0.0, maximum=1.0, value=0.2, step=0.1, interactive=True, label=\"Temperature\",)\n",
        "                    top_p = gr.Slider(\n",
        "                        minimum=0.0, maximum=1.0, value=0.7, step=0.1, interactive=True, label=\"Top P\",)\n",
        "                    max_output_tokens = gr.Slider(\n",
        "                        minimum=0, maximum=1024, value=512, step=64, interactive=True, label=\"Max output tokens\",)\n",
        "\n",
        "            with gr.Column(scale=8):\n",
        "                chatbot = gr.Chatbot(\n",
        "                    elem_id=\"chatbot\", label=\"ShareGPT4V Chatbot\", height=550)\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=8):\n",
        "                        textbox.render()\n",
        "                    with gr.Column(scale=1, min_width=50):\n",
        "                        submit_btn = gr.Button(value=\"Send\", variant=\"primary\")\n",
        "                with gr.Row(elem_id=\"buttons\") as _:\n",
        "                    regenerate_btn = gr.Button(\n",
        "                        value=\"üîÑ  Regenerate\", interactive=True)\n",
        "                    clear_btn = gr.Button(value=\"üóëÔ∏è  Clear\", interactive=True)\n",
        "\n",
        "        gr.Markdown(tos_markdown)\n",
        "        gr.Markdown(learn_more_markdown)\n",
        "        gr.Markdown(ack_markdown)\n",
        "\n",
        "        regenerate_btn.click(\n",
        "            regenerate,\n",
        "            [state, image_process_mode],\n",
        "            [state, chatbot, textbox, imagebox],\n",
        "            queue=False\n",
        "        ).then(\n",
        "            http_bot,\n",
        "            [state, temperature, top_p, max_output_tokens],\n",
        "            [state, chatbot]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_history,\n",
        "            None,\n",
        "            [state, chatbot, textbox, imagebox],\n",
        "            queue=False\n",
        "        )\n",
        "\n",
        "        textbox.submit(\n",
        "            add_text,\n",
        "            [state, textbox, imagebox, image_process_mode],\n",
        "            [state, chatbot, textbox, imagebox],\n",
        "            queue=False\n",
        "        ).then(\n",
        "            http_bot,\n",
        "            [state, temperature, top_p, max_output_tokens],\n",
        "            [state, chatbot]\n",
        "        )\n",
        "\n",
        "        submit_btn.click(\n",
        "            add_text,\n",
        "            [state, textbox, imagebox, image_process_mode],\n",
        "            [state, chatbot, textbox, imagebox],\n",
        "            queue=False\n",
        "        ).then(\n",
        "            http_bot,\n",
        "            [state, temperature, top_p, max_output_tokens],\n",
        "            [state, chatbot]\n",
        "        )\n",
        "\n",
        "        demo.load(\n",
        "            load_demo,\n",
        "            None,\n",
        "            [state],\n",
        "            queue=False\n",
        "        )\n",
        "    return demo\n",
        "\n",
        "model_name = \"llava-v1.5-7b\"\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\"4bit/ShareGPT4V-7B-5GB\", None, \"llava-v1.5-7b\", False, False)\n",
        "demo = build_demo()\n",
        "demo.queue()\n",
        "demo.launch(share=True, inline=False, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXp-FQ-JSw7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}